---
title: "ProjectPWS Summary Writeup"
author: "Jared Casale and Maruthi Ram Nadakuduru"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

ProjectPWS is the package written for STAT290 by Jared Casale and Maruthi Ram 
Nadakuduru. It aimed to fulfil the requirements of the suggested project topic 
relating to querying the Weather Underground website for PWS data. The basic 
requirements were roughly as follows:

1. Provide a means of querying the website to retrieve all PWS near a location 
(e.g. zipcode + distance) or within a region (e.g. a country name). This should 
also be persistable.
2. Store this data in an S4 class (Note: we are using an R6 class for our 
project). This table should have methods to check validity and extract 
subtables based on similar queries from 1.
3. A function to retrieve weather data for these stations, giving a start and 
end time.
4. Function(s) to visualize the data, in particular to view patterns of 
microclimate.
5. A basic computational facility to approximate missing data values.

The following discusses some known issues/quirks and then each requirement is 
addressed with sample code where appropriate.

## Known issues/quirks
### Weather Underground API Key
Use of the API requires an API key. The free tier pricing provides a key with 
severe call limitations, namely 10 calls per minute. Note that each geolookup 
requires a single call and filling a region requires a call to get all cities 
in that region, followed by a call for each city returned to get the stations. 
More restrictive is the fact that the weather history (loading weather data) 
requires a call for each station, for each day in the period. This leads to a 
LOT of calls in a short period. We have provided a key with the package that 
will surely be revoked after too much usage. To work around this, the user would
need to obtain their own key and set the environment variable 
"WUNDERGROUND_API_KEY". All functions would then pick up this variable before 
each new call. The .onAttach function checks for an environment variable with
a key, and sets the default if it is missing. .onDetach will clear this variable
if it is still the default.


### Functions vs R6 methods
We began writing the project with an S4 class, then moved to R6. Unfortunately, 
we couldn't find a great way of documenting the class and chose to leave the 
bulk of our code as separate functions that require one of the R6 objects to be 
passed to it. The result is no different and the object is still the way to 
handle state for all of the actions.

## Requirement 1: Querying for PWS

To load PWS data, we have a method calleed getStations. It queries the 
WUnderground API to get all PWS meeting the specified criteria. Valid queries 
for location are as follows:

* A numeric vector specifying a lat/long pair.
* A US zipcode character string.
* A US state or non-US country name.
* A US state and city pair.
* A non-US country name and city pair.

The API limits to PWS within 40km (around 25 miles) of the zipcode and only the 
first 50 results. An optional numeric mile radius restriction may be supplied, 
limiting to stations within that distance from the specified location. Anything 
greater than 24 will be ignored (due to the API limitation).

Within the code, we query the API and parse the resulting XML to save the 
results of the lookup. If a state or country is supplied without a city, then 
the results of the first lookup give a list of cities within that location. This
function then performs a separate query for each of the region/city pairs and 
concatenates the resulting stations. The following code shows sample queries, 
but requires internet connectivity (skipped in this document):

```{r, eval=FALSE}
library(ProjectPWS)

# Charlotte
latLongStations <- getStations(latlong = c(35.229, -80.8433), radius = 2)
zipStations <- getStations(zip = "90210", radius = 10)
berlinStations <- getStations(country = "Germany", city = "Berlin")
cityStations <- getStations(state = "OR", city = "Portand", radius = 3)
countryStations <- getStations(country = "Spain")
```

## Requirement 2: Store in a class
We started out using an S4 class but preferred to use an R6 class. The class 
PWStations is the container that we load data into. It performs some basic 
validation of the query used to create the table in the initialize() function 
and stores the data used for the query. Calling getStations() will associate a 
data table of PWS stations from the resulting query with the object. Likewise, 
calling loadWeatherData() will create a list of data tables corresponding to the 
weather data retrieved for each station in the query. This list is also 
associated with the original class.

To extract subtables, we have a function called makeStationSubtable(). It 
returns a PWStations object with the specified filter applied to the original 
PWStations. If the original object contained weather data, this will also be 
filtered to the appropriate stations. Filter options are one of the following:

1. Reducing the radius of the original query, so that only stations
2. within the new radius are kept.
3. Choosing a number of closest or farthest stations from the original
4. query location.
5. Choosing a set of stations by name.

The following code will extract some subtables from a preloaded object (output 
suppressed):

```{r, eval=FALSE}
charlotteStations <- getStations(latlong = c(35.229, -80.8433), 
                                 radius = 10)
# Decrease radius to 5 miles
makeStationSubtable(charlotteStations, 
                    newRadius = 5)
# Keep 3 closest
makeStationSubtable(charlotteStations, 
                    numberToKeep = 3) 
# Keep 3 farthest
makeStationSubtable(charlotteStations, 
                    numberToKeep = 3, 
                    nearest = FALSE)
makeStationsSubtable(charlotteStations,
                      stationNames = c("KNCCHARL71", "KNCCHARL83"))
```

## Requirement 3: Load weather data for a time period
Loading weather data is achieved through the loadWeatherData() function. It 
takes a start and end date, as well as a start and end hour (between 0 and 23) 
and retrieves a set of temperature variables for each PWS in the PWStations 
object for that time period. The result is stored as a list of data tables, 
with a measurement for each hour in the specified time period, for the set of 
temperature variables. The default variables are temperature, humidity, 
pressure, wind speed and a plain-text set of conditions. To achieve this, it 
records the first measurement it receives for each hour (data from PWS is 
generally at 5 minute intervals). Any missing hours are filled with NAs. Note 
that the API allows for a query of weather history for a single day at a time. 
As a result, loading of weather data requires a total of (number of stations * 
number of variables) calls, which can rapidly lead to exceeding of free Weather 
Underground API restrictions. To help with this, an optional station limit can 
be provided so that only a fixed number of stations are queried. The function 
will output some progress messages so the user knows something is working.

Some examples of loading weather data are as follows (not run due here):

```{r, eval=FALSE}
stations <- getStations(zip = "98107", radius = 2)
weatherData <- loadWeatherData(stations, startDate = "3/3/2015",
                               startHour = 9,
                               endHour = 5,
                               stationLimit = 1)

# Just temperature and humidity
# Startdate defaults to 0, endDate to current date
weatherData2 <- loadWeatherData(stations, 
                                startDate = "3/1/2015",
                                endDate = "3/3/2015", 
                                weatherVars = c("tempi",
                                                "hum")
                                stationLimit = 3)
```

## Requirement 4: Visualizing the data

## Requirement 5: Approximating missing data
Missing data did not seem to be a huge issue since we are limiting to hourly 
intervals and PWS generally seemed to provide data for each hour that we 
queried. However we did build a function called validateTrimAndFill which does 
three things:

1. Check all values and determine if they are within a (generously) reasonable 
range. If they are not, they will be replaced with an NA.
2. Trim each table so that any columns with less than 2 data points are removed.
3. Fill in missing values by taking the mean of 5 imputations using the amelia 
function.

The valid ranges were determined loosely, by examining lowest/highest recorded 
values. This means that we will only find obvious problems, which is fair. 
Using Amelia to fill values may not necessarily be appropriate, but it allows 
the user to easily fill in missing values. Five imputations are performed of the
dataset and values are the mean of these imputations. It would be up to the user
to decide a more rigorous model for interpolation/estimation.

Some sample code is as follows (included in examples):

```{r}
library(ProjectPWS)
data(charlotteStations)

# Just keep two stations
charlotteStations <- makeStationSubtable(charlotteStations,
                       stationNames = c("KNCCHARL71", "KNCCHARL83"))

# Load data for these stations for a single day
loadWeatherData(charlotteStations, startDate = "3/15/2015")

# Just validate
validateTrimAndFill(charlotteStations, stopAfterValidation = TRUE)

# Validate then trim
validateTrimAndFill(charlotteStations, stopAfterTrim = TRUE)

# Validate, trim and fill missing data
validateTrimAndFill(charlotteStations)
```

## Testing
testthat is used to run automated tests on each of the functions (outside of 
shiny). It does some basic tests of invalid input as well as the core 
functionality described here. We limit testing because of the number of calls 
that generally have to be made and would have to invest in a more rigorous 
testing scheme if we were to continue here i.e. improve the design so that we 
could shim the API and run many more tests locally etc.

